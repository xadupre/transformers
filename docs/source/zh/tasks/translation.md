<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ç¿»è¯‘

[[open-in-colab]]

<Youtube id="1JvfrvZgi6c"/>

ç¿»è¯‘å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚å®ƒæ˜¯å¯ä»¥è¡¨è¿°ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜çš„å‡ ä¸ªä»»åŠ¡ä¹‹ä¸€â€”â€”è¿™æ˜¯ä¸€ç§ä»è¾“å…¥è¿”å›æŸäº›è¾“å‡ºçš„å¼ºå¤§æ¡†æ¶ï¼Œé€‚ç”¨äºç¿»è¯‘æˆ–æ‘˜è¦ç­‰ä»»åŠ¡ã€‚ç¿»è¯‘ç³»ç»Ÿé€šå¸¸ç”¨äºä¸åŒè¯­è¨€æ–‡æœ¬ä¹‹é—´çš„è½¬æ¢ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨äºè¯­éŸ³ï¼Œæˆ–è€…æ–‡æœ¬è½¬è¯­éŸ³ã€è¯­éŸ³è½¬æ–‡æœ¬ç­‰ç»„åˆåœºæ™¯ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨ [OPUS Books](https://huggingface.co/datasets/opus_books) æ•°æ®é›†çš„è‹±æ³•å­é›†ä¸Šå¾®è°ƒ [T5](https://huggingface.co/google-t5/t5-small)ï¼Œå°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆæ³•æ–‡ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

<Tip>

å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ‰€æœ‰ä¸æœ¬ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæœ€å¥½æŸ¥çœ‹[ä»»åŠ¡é¡µ](https://huggingface.co/tasks/translation)ã€‚

</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate sacrebleu
```

å»ºè®®æ‚¨ç™»å½• Hugging Face è´¦æˆ·ï¼Œä»¥ä¾¿å°†æ¨¡å‹ä¸Šä¼ å¹¶åˆ†äº«ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ OPUS Books æ•°æ®é›†

é¦–å…ˆä» ğŸ¤— Datasets åº“ä¸­åŠ è½½ [OPUS Books](https://huggingface.co/datasets/opus_books) æ•°æ®é›†çš„è‹±æ³•å­é›†ï¼š

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

ä½¿ç”¨ [`~datasets.Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}
```

`translation`ï¼šæ–‡æœ¬çš„è‹±æ–‡å’Œæ³•æ–‡ç¿»è¯‘ã€‚

## é¢„å¤„ç†

<Youtube id="XAR8jnZZuUs"/>

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ T5 åˆ†è¯å™¨ï¼Œå¤„ç†è‹±æ³•è¯­è¨€å¯¹ï¼š

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

æ‚¨è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦ï¼š

1. åœ¨è¾“å…¥å‰æ·»åŠ æç¤ºè¯ï¼Œè®© T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡ã€‚æŸäº›èƒ½å¤Ÿå¤„ç†å¤šç§ NLP ä»»åŠ¡çš„æ¨¡å‹éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡æç¤ºã€‚
2. åœ¨ `text_target` å‚æ•°ä¸­è®¾ç½®ç›®æ ‡è¯­è¨€ï¼ˆæ³•è¯­ï¼‰ï¼Œä»¥ç¡®ä¿åˆ†è¯å™¨èƒ½æ­£ç¡®å¤„ç†ç›®æ ‡æ–‡æœ¬ã€‚å¦‚æœä¸è®¾ç½® `text_target`ï¼Œåˆ†è¯å™¨ä¼šå°†ç›®æ ‡æ–‡æœ¬ä½œä¸ºè‹±è¯­å¤„ç†ã€‚
3. å°†åºåˆ—æˆªæ–­è‡³ä¸è¶…è¿‡ `max_length` å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

ä½¿ç”¨ ğŸ¤— Datasets çš„ [`~datasets.Dataset.map`] æ–¹æ³•å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡è®¾ç½® `batched=True` ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¯ä»¥åŠ é€Ÿ `map` å‡½æ•°ï¼š

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

ç°åœ¨ä½¿ç”¨ [`DataCollatorForSeq2Seq`] åˆ›å»ºä¸€æ‰¹æ ·æœ¬ã€‚åœ¨æ•´ç†æ—¶å°†å¥å­*åŠ¨æ€å¡«å……*è‡³æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œæ¯”å°†æ•´ä¸ªæ•°æ®é›†å¡«å……è‡³æœ€å¤§é•¿åº¦æ›´é«˜æ•ˆã€‚

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥è¯„ä¼°æŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) æŒ‡æ ‡ï¼ˆå‚é˜… ğŸ¤— Evaluate [å¿«é€Ÿæ•™ç¨‹](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼Œäº†è§£æ›´å¤šå…³äºåŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ï¼š

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹ç»“æœå’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®— SacreBLEU åˆ†æ•°ï¼š

```py
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œåœ¨è®¾ç½®è®­ç»ƒæ—¶ä¼šç”¨åˆ°å®ƒã€‚

## è®­ç»ƒ

<Tip>

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ [`Trainer`] å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](../training#train-with-pytorch-trainer)çš„åŸºç¡€æ•™ç¨‹ï¼

</Tip>

ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForSeq2SeqLM`] åŠ è½½ T5ï¼š

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`Seq2SeqTrainingArguments`] ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True`ï¼Œå°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° SacreBLEU æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ [`Seq2SeqTrainer`]ï¼ŒåŒæ—¶ä¼ å…¥æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ã€‚
3. è°ƒç”¨ [`~Trainer.train`] å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True, #change to bf16=True for XPU
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œè®©æ‰€æœ‰äººéƒ½èƒ½ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```

<Tip>

å¦‚éœ€äº†è§£å¦‚ä½•å¾®è°ƒç¿»è¯‘æ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„
[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)ã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼

å‡†å¤‡ä¸€äº›æ‚¨æƒ³è¦ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‰€å¤„ç†çš„ä»»åŠ¡ä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ã€‚å¯¹äºä»è‹±è¯­åˆ°æ³•è¯­çš„ç¿»è¯‘ï¼Œå‰ç¼€å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å°† `input_ids` ä½œä¸º PyTorch å¼ é‡è¿”å›ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

ä½¿ç”¨ [`~generation.GenerationMixin.generate`] æ–¹æ³•åˆ›å»ºç¿»è¯‘ç»“æœã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆå‚æ•°çš„æ›´å¤šè¯¦æƒ…ï¼Œè¯·æŸ¥é˜…[æ–‡æœ¬ç”Ÿæˆ](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("username/my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

å°†ç”Ÿæˆçš„è¯å…ƒ id è§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'
```
