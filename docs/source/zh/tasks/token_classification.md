<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# è¯å…ƒåˆ†ç±»

[[open-in-colab]]

<Youtube id="wVHdVlPScxA"/>

è¯å…ƒåˆ†ç±»ä¸ºå¥å­ä¸­çš„æ¯ä¸ªè¯å…ƒåˆ†é…æ ‡ç­¾ã€‚æœ€å¸¸è§çš„è¯å…ƒåˆ†ç±»ä»»åŠ¡ä¹‹ä¸€æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚NER å°è¯•ä¸ºå¥å­ä¸­çš„æ¯ä¸ªå®ä½“æ‰¾åˆ°å¯¹åº”æ ‡ç­¾ï¼Œä¾‹å¦‚äººåã€åœ°åæˆ–ç»„ç»‡åã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨ [WNUT 17](https://huggingface.co/datasets/wnut_17) æ•°æ®é›†ä¸Šå¾®è°ƒ [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased)ï¼Œä»¥æ£€æµ‹æ–°å…´å®ä½“ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

<Tip>

å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ‰€æœ‰ä¸æœ¬ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæœ€å¥½æŸ¥çœ‹[ä»»åŠ¡é¡µ](https://huggingface.co/tasks/token-classification)ã€‚

</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate seqeval
```

å»ºè®®æ‚¨ç™»å½• Hugging Face è´¦æˆ·ï¼Œä»¥ä¾¿å°†æ¨¡å‹ä¸Šä¼ å¹¶åˆ†äº«ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ WNUT 17 æ•°æ®é›†

é¦–å…ˆä» ğŸ¤— Datasets åº“ä¸­åŠ è½½ WNUT 17 æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
}
```

`ner_tags` ä¸­çš„æ¯ä¸ªæ•°å­—ä»£è¡¨ä¸€ä¸ªå®ä½“ã€‚å°†æ•°å­—è½¬æ¢ä¸ºæ ‡ç­¾åç§°ï¼Œä»¥äº†è§£å®ä½“ç±»å‹ï¼š

```py
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

æ¯ä¸ª `ner_tag` çš„å‰ç¼€å­—æ¯è¡¨ç¤ºå®ä½“ä¸­è¯å…ƒçš„ä½ç½®ï¼š

- `B-` è¡¨ç¤ºå®ä½“çš„å¼€å§‹ã€‚
- `I-` è¡¨ç¤ºè¯å…ƒåŒ…å«åœ¨åŒä¸€å®ä½“ä¸­ï¼ˆä¾‹å¦‚ï¼Œ`State` è¯å…ƒæ˜¯ `Empire State Building` ç­‰å®ä½“çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
- `0` è¡¨ç¤ºè¯¥è¯å…ƒä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚

## é¢„å¤„ç†

<Youtube id="iY2AZYdZAr0"/>

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ DistilBERT åˆ†è¯å™¨ï¼Œå¯¹ `tokens` å­—æ®µè¿›è¡Œé¢„å¤„ç†ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

å¦‚ä¸Šé¢ç¤ºä¾‹çš„ `tokens` å­—æ®µæ‰€ç¤ºï¼Œçœ‹èµ·æ¥è¾“å…¥å·²ç»å®Œæˆäº†åˆ†è¯ã€‚ä½†å®é™…ä¸Šè¾“å…¥å°šæœªåˆ†è¯ï¼Œæ‚¨éœ€è¦è®¾ç½® `is_split_into_words=True` å°†è¯è¯­åˆ†è¯ä¸ºå­è¯ã€‚ä¾‹å¦‚ï¼š

```py
>>> example = wnut["train"][0]
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

ç„¶è€Œï¼Œè¿™ä¼šæ·»åŠ ä¸€äº›ç‰¹æ®Šè¯å…ƒ `[CLS]` å’Œ `[SEP]`ï¼Œå­è¯åˆ†è¯ä¼šé€ æˆè¾“å…¥ä¸æ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…â€”â€”åŸæœ¬å¯¹åº”å•ä¸ªæ ‡ç­¾çš„å•ä¸ªè¯ï¼Œç°åœ¨å¯èƒ½è¢«åˆ†å‰²ä¸ºä¸¤ä¸ªå­è¯ã€‚æ‚¨éœ€è¦é€šè¿‡ä»¥ä¸‹æ–¹å¼é‡æ–°å¯¹é½è¯å…ƒå’Œæ ‡ç­¾ï¼š

1. ä½¿ç”¨ [`word_ids`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_ids) æ–¹æ³•å°†æ‰€æœ‰è¯å…ƒæ˜ å°„åˆ°å¯¹åº”çš„è¯è¯­ã€‚
2. å¯¹ç‰¹æ®Šè¯å…ƒ `[CLS]` å’Œ `[SEP]` åˆ†é…æ ‡ç­¾ `-100`ï¼Œä½¿å…¶è¢« PyTorch çš„æŸå¤±å‡½æ•°å¿½ç•¥ï¼ˆå‚è§ [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)ï¼‰ã€‚
3. ä»…ä¸ºç»™å®šè¯è¯­çš„ç¬¬ä¸€ä¸ªè¯å…ƒæ‰“æ ‡ç­¾ï¼Œå¯¹åŒä¸€è¯è¯­çš„å…¶ä»–å­è¯å…ƒåˆ†é… `-100`ã€‚

ä¸‹é¢æ˜¯åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é‡æ–°å¯¹é½è¯å…ƒå’Œæ ‡ç­¾ã€å¹¶å°†åºåˆ—æˆªæ–­è‡³ä¸è¶…è¿‡ DistilBERT æœ€å¤§è¾“å…¥é•¿åº¦çš„æ–¹æ³•ï¼š

```py
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # å°†è¯å…ƒæ˜ å°„åˆ°å¯¹åº”è¯è¯­
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # å°†ç‰¹æ®Šè¯å…ƒè®¾ç½®ä¸º -100
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # ä»…ä¸ºç»™å®šè¯è¯­çš„ç¬¬ä¸€ä¸ªè¯å…ƒæ‰“æ ‡ç­¾
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

ä½¿ç”¨ ğŸ¤— Datasets çš„ [`~datasets.Dataset.map`] å‡½æ•°å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡è®¾ç½® `batched=True` ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¯ä»¥åŠ é€Ÿ `map` å‡½æ•°ï¼š

```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

ç°åœ¨ä½¿ç”¨ [`DataCollatorWithPadding`] åˆ›å»ºä¸€æ‰¹æ ·æœ¬ã€‚åœ¨æ•´ç†æ—¶å°†å¥å­*åŠ¨æ€å¡«å……*è‡³æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œæ¯”å°†æ•´ä¸ªæ•°æ®é›†å¡«å……è‡³æœ€å¤§é•¿åº¦æ›´é«˜æ•ˆã€‚

```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥è¯„ä¼°æŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) æ¡†æ¶ï¼ˆå‚é˜… ğŸ¤— Evaluate [å¿«é€Ÿæ•™ç¨‹](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼Œäº†è§£æ›´å¤šå…³äºåŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ã€‚seqeval å®é™…ä¸Šä¼šäº§ç”Ÿå¤šä¸ªåˆ†æ•°ï¼šç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 å’Œå‡†ç¡®ç‡ã€‚

```py
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

é¦–å…ˆè·å– NER æ ‡ç­¾ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†çœŸå®é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®—åˆ†æ•°ï¼š

```py
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œåœ¨è®¾ç½®è®­ç»ƒæ—¶ä¼šç”¨åˆ°å®ƒã€‚

## è®­ç»ƒ

åœ¨å¼€å§‹è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œä½¿ç”¨ `id2label` å’Œ `label2id` åˆ›å»ºé¢„æœŸ id åˆ°å…¶æ ‡ç­¾çš„æ˜ å°„ï¼š

```py
>>> id2label = {
...     0: "O",
...     1: "B-corporation",
...     2: "I-corporation",
...     3: "B-creative-work",
...     4: "I-creative-work",
...     5: "B-group",
...     6: "I-group",
...     7: "B-location",
...     8: "I-location",
...     9: "B-person",
...     10: "I-person",
...     11: "B-product",
...     12: "I-product",
... }
>>> label2id = {
...     "O": 0,
...     "B-corporation": 1,
...     "I-corporation": 2,
...     "B-creative-work": 3,
...     "I-creative-work": 4,
...     "B-group": 5,
...     "I-group": 6,
...     "B-location": 7,
...     "I-location": 8,
...     "B-person": 9,
...     "I-person": 10,
...     "B-product": 11,
...     "I-product": 12,
... }
```

<Tip>

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ [`Trainer`] å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](../training#train-with-pytorch-trainer)çš„åŸºç¡€æ•™ç¨‹ï¼

</Tip>

ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForTokenClassification`] åŠ è½½ DistilBERTï¼Œå¹¶æŒ‡å®šé¢„æœŸæ ‡ç­¾æ•°é‡å’Œæ ‡ç­¾æ˜ å°„ï¼š

```py
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True`ï¼Œå°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° seqeval åˆ†æ•°å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ [`Trainer`]ï¼ŒåŒæ—¶ä¼ å…¥æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ã€‚
3. è°ƒç”¨ [`~Trainer.train`] å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_wnut_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_wnut["train"],
...     eval_dataset=tokenized_wnut["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œè®©æ‰€æœ‰äººéƒ½èƒ½ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```

<Tip>

å¦‚éœ€äº†è§£å¦‚ä½•å¾®è°ƒè¯å…ƒåˆ†ç±»æ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„
[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)ã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼

å‡†å¤‡ä¸€äº›æ‚¨æƒ³è¦è¿›è¡Œæ¨æ–­çš„æ–‡æœ¬ï¼š

```py
>>> text = "The Golden State Warriors are an American professional basketball team based in San Francisco."
```

ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­æœ€ç®€å•çš„æ–¹å¼æ˜¯åœ¨ [`pipeline`] ä¸­ä½¿ç”¨å®ƒã€‚ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ª NER `pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> classifier = pipeline("ner", model="stevhliu/my_awesome_wnut_model")
>>> classifier(text)
[{'entity': 'B-location',
  'score': 0.42658573,
  'index': 2,
  'word': 'golden',
  'start': 4,
  'end': 10},
 {'entity': 'I-location',
  'score': 0.35856336,
  'index': 3,
  'word': 'state',
  'start': 11,
  'end': 16},
 {'entity': 'B-group',
  'score': 0.3064001,
  'index': 4,
  'word': 'warriors',
  'start': 17,
  'end': 25},
 {'entity': 'B-location',
  'score': 0.65523505,
  'index': 13,
  'word': 'san',
  'start': 80,
  'end': 83},
 {'entity': 'B-location',
  'score': 0.4668663,
  'index': 14,
  'word': 'francisco',
  'start': 84,
  'end': 93}]
```

å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤ç° `pipeline` çš„ç»“æœï¼š

å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è¿”å› PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› `logits`ï¼š

```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

è·å–æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„ `id2label` æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾ï¼š

```py
>>> predictions = torch.argmax(logits, dim=2)
>>> predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]
>>> predicted_token_class
['O',
 'O',
 'B-location',
 'I-location',
 'B-group',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'B-location',
 'B-location',
 'O',
 'O']
```
