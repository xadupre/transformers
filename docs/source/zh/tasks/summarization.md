<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# æ‘˜è¦

[[open-in-colab]]

<Youtube id="yHnr5Dk2zCI"/>

æ‘˜è¦ä»»åŠ¡ç”Ÿæˆæ–‡æ¡£æˆ–æ–‡ç« çš„ç®€çŸ­ç‰ˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰é‡è¦ä¿¡æ¯ã€‚ä¸ç¿»è¯‘ç±»ä¼¼ï¼Œå®ƒæ˜¯å¦ä¸€ä¸ªå¯ä»¥è¡¨è¿°ä¸ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„ä¾‹å­ã€‚æ‘˜è¦å¯ä»¥åˆ†ä¸ºï¼š

- æŠ½å–å¼ï¼šä»æ–‡æ¡£ä¸­æå–æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
- ç”Ÿæˆå¼ï¼šç”Ÿæˆèƒ½å¤Ÿæ•è·æœ€é‡è¦ä¿¡æ¯çš„æ–°æ–‡æœ¬ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨ [BillSum](https://huggingface.co/datasets/billsum) æ•°æ®é›†çš„åŠ åˆ©ç¦å°¼äºšå·æ³•æ¡ˆå­é›†ä¸Šå¾®è°ƒ [T5](https://huggingface.co/google-t5/t5-small)ï¼Œç”¨äºç”Ÿæˆå¼æ‘˜è¦ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

<Tip>

å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ‰€æœ‰ä¸æœ¬ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæœ€å¥½æŸ¥çœ‹[ä»»åŠ¡é¡µ](https://huggingface.co/tasks/summarization)ã€‚

</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate rouge_score
```

å»ºè®®æ‚¨ç™»å½• Hugging Face è´¦æˆ·ï¼Œä»¥ä¾¿å°†æ¨¡å‹ä¸Šä¼ å¹¶åˆ†äº«ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ BillSum æ•°æ®é›†

é¦–å…ˆä» ğŸ¤— Datasets åº“ä¸­åŠ è½½ BillSum æ•°æ®é›†ä¸­è¾ƒå°çš„åŠ åˆ©ç¦å°¼äºšå·æ³•æ¡ˆå­é›†ï¼š

```py
>>> from datasets import load_dataset

>>> billsum = load_dataset("billsum", split="ca_test")
```

ä½¿ç”¨ [`~datasets.Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> billsum = billsum.train_test_split(test_size=0.2)
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> billsum["train"][0]
{'summary': 'Existing law authorizes state agencies to enter into contracts for the acquisition of goods or services upon approval by the Department of General Services. Existing law sets forth various requirements and prohibitions for those contracts, including, but not limited to, a prohibition on entering into contracts for the acquisition of goods or services of $100,000 or more with a contractor that discriminates between spouses and domestic partners or same-sex and different-sex couples in the provision of benefits. Existing law provides that a contract entered into in violation of those requirements and prohibitions is void and authorizes the state or any person acting on behalf of the state to bring a civil action seeking a determination that a contract is in violation and therefore void. Under existing law, a willful violation of those requirements and prohibitions is a misdemeanor.\nThis bill would also prohibit a state agency from entering into contracts for the acquisition of goods or services of $100,000 or more with a contractor that discriminates between employees on the basis of gender identity in the provision of benefits, as specified. By expanding the scope of a crime, this bill would impose a state-mandated local program.\nThe California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\nThis bill would provide that no reimbursement is required by this act for a specified reason.',
 'text': 'The people of the State of California do enact as follows: ...',
 'title': 'An act to add Section 10295.35 to the Public Contract Code, relating to public contracts.'}
```

æ‚¨ä¼šç”¨åˆ°çš„ä¸¤ä¸ªå­—æ®µæ˜¯ï¼š

- `text`ï¼šæ³•æ¡ˆæ–‡æœ¬ï¼Œå°†ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚
- `summary`ï¼š`text` çš„ç²¾ç®€ç‰ˆæœ¬ï¼Œå°†ä½œä¸ºæ¨¡å‹çš„ç›®æ ‡è¾“å‡ºã€‚

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ T5 åˆ†è¯å™¨ï¼Œå¤„ç† `text` å’Œ `summary`ï¼š

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

æ‚¨è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦ï¼š

1. åœ¨è¾“å…¥å‰æ·»åŠ æç¤ºè¯ï¼Œè®© T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚æŸäº›èƒ½å¤Ÿå¤„ç†å¤šç§ NLP ä»»åŠ¡çš„æ¨¡å‹éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡æç¤ºã€‚
2. åœ¨å¯¹æ ‡ç­¾è¿›è¡Œåˆ†è¯æ—¶ä½¿ç”¨å…³é”®å­—å‚æ•° `text_target`ã€‚
3. å°†åºåˆ—æˆªæ–­è‡³ä¸è¶…è¿‡ `max_length` å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚

```py
>>> prefix = "summarize: "


>>> def preprocess_function(examples):
...     inputs = [prefix + doc for doc in examples["text"]]
...     model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

...     labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True)

...     model_inputs["labels"] = labels["input_ids"]
...     return model_inputs
```

ä½¿ç”¨ ğŸ¤— Datasets çš„ [`~datasets.Dataset.map`] æ–¹æ³•å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡è®¾ç½® `batched=True` ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¯ä»¥åŠ é€Ÿ `map` å‡½æ•°ï¼š

```py
>>> tokenized_billsum = billsum.map(preprocess_function, batched=True)
```

ç°åœ¨ä½¿ç”¨ [`DataCollatorForSeq2Seq`] åˆ›å»ºä¸€æ‰¹æ ·æœ¬ã€‚åœ¨æ•´ç†æ—¶å°†å¥å­*åŠ¨æ€å¡«å……*è‡³æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œæ¯”å°†æ•´ä¸ªæ•°æ®é›†å¡«å……è‡³æœ€å¤§é•¿åº¦æ›´é«˜æ•ˆã€‚

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥è¯„ä¼°æŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) æŒ‡æ ‡ï¼ˆå‚é˜… ğŸ¤— Evaluate [å¿«é€Ÿæ•™ç¨‹](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼Œäº†è§£æ›´å¤šå…³äºåŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ï¼š

```py
>>> import evaluate

>>> rouge = evaluate.load("rouge")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹ç»“æœå’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®— ROUGE æŒ‡æ ‡ï¼š

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
...     result["gen_len"] = np.mean(prediction_lens)

...     return {k: round(v, 4) for k, v in result.items()}
```

æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œåœ¨è®¾ç½®è®­ç»ƒæ—¶ä¼šç”¨åˆ°å®ƒã€‚

## è®­ç»ƒ

<Tip>

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ [`Trainer`] å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](../training#train-with-pytorch-trainer)çš„åŸºç¡€æ•™ç¨‹ï¼

</Tip>

ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForSeq2SeqLM`] åŠ è½½ T5ï¼š

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`Seq2SeqTrainingArguments`] ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True`ï¼Œå°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° ROUGE æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ [`Seq2SeqTrainer`]ï¼ŒåŒæ—¶ä¼ å…¥æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ã€‚
3. è°ƒç”¨ [`~Trainer.train`] å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_billsum_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=4,
...     predict_with_generate=True,
...     fp16=True, #change to bf16=True for XPU
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_billsum["train"],
...     eval_dataset=tokenized_billsum["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œè®©æ‰€æœ‰äººéƒ½èƒ½ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```

<Tip>

å¦‚éœ€äº†è§£å¦‚ä½•å¾®è°ƒæ‘˜è¦æ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„
[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)ã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼

å‡†å¤‡ä¸€äº›æ‚¨æƒ³è¦ç”Ÿæˆæ‘˜è¦çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‰€å¤„ç†çš„ä»»åŠ¡ä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ã€‚å¯¹äºæ‘˜è¦ä»»åŠ¡ï¼Œå‰ç¼€å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
>>> text = "summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes."
```

å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å°† `input_ids` ä½œä¸º PyTorch å¼ é‡è¿”å›ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_billsum_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

ä½¿ç”¨ [`~generation.GenerationMixin.generate`] æ–¹æ³•åˆ›å»ºæ‘˜è¦ã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆå‚æ•°çš„æ›´å¤šè¯¦æƒ…ï¼Œè¯·æŸ¥é˜…[æ–‡æœ¬ç”Ÿæˆ](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("username/my_awesome_billsum_model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)
```

å°†ç”Ÿæˆçš„è¯å…ƒ id è§£ç å›æ–‡æœ¬ï¼š

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it\'s the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'
```
